{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests                         # Requests the HTTP to fetch the web pages.\n",
    "from bs4 import BeautifulSoup           # For web scraping, or parsing HTML and XML documents.\n",
    "import pandas as pd                     # Data manipulation in tabular form.\n",
    "import math                             # Quick mathematical functions for calculations.\n",
    "from sqlalchemy import create_engine    # To create connection to the SQL database.\n",
    "import pyodbc                           # For connecting to databases using ODBC driver.\n",
    "import concurrent.futures               # Makes the web scraping process faster.\n",
    "import logging                          # To log web scrap.\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scrap the car links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are extracting used car data from Truecar.com. \n",
    "# I configured the car brand, model as per my requirement. \n",
    "# If you don't know what model you want, we can leave that space blank too as ' '. \n",
    "# Setting a realistic search radius like 100 will show limited but easy to check listings.\n",
    "# Blank radius will show within 75 miles of the zip code. \n",
    "\n",
    "car_brand = 'honda'\n",
    "car_model = 'pilot'\n",
    "zip_code = '78628'\n",
    "radius = '100'\n",
    "base_url = f'https://www.truecar.com/used-cars-for-sale/listings/{car_brand}/{car_model}/location-{zip_code}/?searchRadius={radius}&page='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEADERS helps to avoid any kind of blocks from the website. It mimics a typical browser request.\n",
    "# User_Agent for a computer can be found on: https://www.whatsmyua.info/\n",
    "# Accept-Language used here is English.\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the above web page is loaded, the website displays available cars in blocks.\n",
    "# Clicking each block will open a new link that has details about the car. \n",
    "# Now, extrating each link from the listings.\n",
    "\n",
    "# Fetch car links from each listing.\n",
    "# Function is used here to avoid duplication, and let the code fetch all the links listed in the page.\n",
    "# New links are stored as 'url' which will be used later. \n",
    "\n",
    "def fetch_page(page, session): # Each car link is retrieved from individual listing blocks.\n",
    "    try:\n",
    "        url = f\"{base_url}{page}\"\n",
    "        response = session.get(url, headers=HEADERS) # Sends request to the website\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser') # 'Soup' Parses the above response to extract the html contents\n",
    "        return [f\"https://www.truecar.com{car['href']}\" for car in soup.find_all('a', class_='linkable card-overlay order-2')]\n",
    "        # By inspecting the web page, <a> tags were specified to the car lisiting, so this was used. \n",
    "\n",
    "    except requests.exceptions.RequestException:\n",
    "        return []\n",
    "\n",
    "# Sends a GET request to the specified URL using session-level connections to reuse settings like headers, \n",
    "# improving performance and ensuring consistency across multiple requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Scrap data from each car listing link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each url was extracted in the above step.\n",
    "# Now, each url is inspected, and the key data points are extracted like Title, price, miles, date listed, and so on.  \n",
    "# Class, tags, strings are inspected to extract the key data points and divided for easier readability.\n",
    "\n",
    "def scrape_car_details(url, session):\n",
    "    try:\n",
    "        response = session.get(url)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Title\n",
    "            title = soup.find('h1', {'data-test': \"marketplaceVdpHeader\"}).text.strip()\n",
    "            # Mileage  \n",
    "            mileage = soup.find('span', class_='shrink-0 flex items-center').text.strip()\n",
    "            # Price  \n",
    "            price = soup.find('div', {'data-test': 'unifiedPricingInfoPrice'}).text.strip()  \n",
    "            # VIN\n",
    "            vin_element = soup.find(string=\"VIN:\")\n",
    "            VIN = vin_element.find_parent(\"div\").get_text(strip=True).replace(\"VIN:\", \"\") if vin_element else \"N/A\"\n",
    "            # Color\n",
    "            color = soup.find(string=\"Exterior:\")\n",
    "            color = color.find_parent(\"div\").get_text(strip=True).replace(\"Exterior:\", \"\") if color else \"N/A\"\n",
    "            # Location using the <p> tag\n",
    "            location_element = soup.find('p', class_='mr-1')\n",
    "            location = location_element.get_text(strip=True) if location_element else \"N/A\"\n",
    "            # Listed Date\n",
    "            listed_element = soup.find(string=lambda text: text and \"Listed\" in text)\n",
    "            listed_date = listed_element.strip().replace(\"Listed \", \"\") if listed_element else \"N/A\"\n",
    "            # AWD\n",
    "            awd_element = soup.find('div', class_='flex items-center')\n",
    "            awd = awd_element.get_text(strip=True) if awd_element else \"N/A\"\n",
    "            # Number of accidents   \n",
    "            accident_element = soup.find(string=lambda text: text and \"Accidents\" in text)\n",
    "            accident = accident_element.get_text(strip=True) if accident_element else \"N/A\"\n",
    "            # Number of owners\n",
    "            owner_element = soup.find(string=lambda text: text and \"Owner\" in text)\n",
    "            owner = owner_element.get_text(strip=True) if owner_element else \"N/A\"\n",
    "            # Car Title type\n",
    "            clean_element = soup.find(string=lambda text: text and \"Title\" in text)\n",
    "            title_type = clean_element.get_text(strip=True) if clean_element else \"N/A\"\n",
    "            # Fuel type\n",
    "            fuel = soup.find(string=\"Fuel Type:\")\n",
    "            fuel = fuel.find_parent(\"div\").get_text(strip=True).replace(\"Fuel Type:\", \"\") if fuel else \"N/A\"\n",
    "            # Fuel Efficiency\n",
    "            eff_element = soup.find(string=lambda text: text and \"city / \" in text)\n",
    "            efficiency = eff_element.get_text(strip=True) if eff_element else \"N/A\"\n",
    "                \n",
    "            # Return the extracted data\n",
    "            return {\n",
    "                'Title': title,\n",
    "                'Mileage': mileage,\n",
    "                'Price': price,\n",
    "                'URL': url,\n",
    "                'VIN': VIN,\n",
    "                'Color': color,\n",
    "                'Location': location,\n",
    "                'Listed': listed_date,\n",
    "                'Accidents': accident,\n",
    "                'Owners': owner,\n",
    "                'Fuel_Type': fuel,\n",
    "                'Fuel Efficiency': efficiency,\n",
    "                'Car_Title': title_type\n",
    "            }\n",
    "    except requests.exceptions.RequestException:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This establishes stable connection for repeated and consistent requests.\n",
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total listings: 438, Pages: 15\n"
     ]
    }
   ],
   "source": [
    "# This step is to calculate the number of listings, which is displayed in the initial web link in step 1.\n",
    "# Number of listing is shown before it starts to fetch data from each link.\n",
    "# It is useful to see how many links and listing are being fetched.\n",
    "# If the listing is more than 10,000, it takes time so we can cancel the operation without wasting time.\n",
    "# Concurrent is for optimized and faster web scarping, used to simultaneouly fetch data from multiple car links.\n",
    "# Main data scraping, that takes time, happens in this step.\n",
    "\n",
    "# Counts the total number of listing here\n",
    "try:\n",
    "    initial_response = session.get(f'{base_url}1', headers=HEADERS)\n",
    "    initial_response.raise_for_status()\n",
    "    soup = BeautifulSoup(initial_response.content, 'html.parser')\n",
    "    total_listings = int(soup.find('span', {'data-test': 'marketplaceSrpListingsTotalCount'}).text.replace(',', '').strip())\n",
    "    total_pages = math.ceil(total_listings / 30)\n",
    "\n",
    "    print(f\"Total listings: {total_listings}, Pages: {total_pages}\")\n",
    "\n",
    "    # Fetch car links concurrently\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        car_links = [link for result in executor.map(fetch_page, range(1, total_pages + 1), [session]*total_pages) for link in result]\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching initial page: {e}\")\n",
    "\n",
    "# Scraps each link here\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    car_data_list = [result for result in executor.map(scrape_car_details, car_links, [session]*len(car_links)) if result]\n",
    "\n",
    "# Panda package to convert the above extracted data to a dataframe for further manipulation\n",
    "car_data_df = pd.DataFrame(car_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To log the web scraping time.\n",
    "\n",
    "# Defining a log file\n",
    "log_file_path = '/Users/dris/Desktop/SQL-ETL/Car Prices Project/GitHub/ETL/web_scraping.log'\n",
    "\n",
    "# If the directory doesn't exists, this creates it.\n",
    "os.makedirs(os.path.dirname(log_file_path), exist_ok=True)\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(\n",
    "    filename='/Users/dris/Desktop/SQL-ETL/Car Prices Project/GitHub/ETL/web_scraping.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(message)s'\n",
    ")\n",
    "logging.info(f\"Script ran. Total listings: {total_listings}, Pages: {total_pages}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates for VIN, \n",
    "# As each car has a unique VIN, this step removes all the duplicate entries\n",
    "car_data_df = car_data_df.drop_duplicates(subset='VIN')\n",
    "\n",
    "# car_data_df.dtypes # use this to explore the data type which will be needed for manipulation\n",
    "\n",
    "# Extracting the needed information, like adding new columns, extracting model details, miles, fuel efficiency,\n",
    "# converting price to float variable, splitting the strings, keeping only the needed word or numbers and so on.\n",
    "# Example: Keeping only numeric form. Before the entry had '55340 miles', converting it to '55340'.\n",
    "car_data_df.insert(0, 'SNum', range(1, len(car_data_df) + 1))\n",
    "car_data_df[['Status', 'Miles']] = car_data_df['Mileage'].str.split('Â·', expand=True)\n",
    "car_data_df['Miles'] = car_data_df['Miles'].str.replace('miles', '').str.replace(',', '').str.strip().astype(int)\n",
    "car_data_df['Year'] = car_data_df['Title'].str.split().str[0]\n",
    "car_data_df['Brand'] = car_data_df['Title'].str.split().str[1]\n",
    "car_data_df['AWD'] = car_data_df['Title'].str.split().str[-1].apply(lambda x: 'Yes' if x == 'AWD' else 'No')\n",
    "car_data_df['Model'] = car_data_df['Title'].str.split().str[2:3].str.join(' ')\n",
    "car_data_df['Adv_Model'] = car_data_df['Title'].str.split().str[3:].apply(lambda x: ' '.join(x))\n",
    "car_data_df[['City', 'State']] = car_data_df['Location'].str.split(', ', expand=True)\n",
    "car_data_df['Accidents'] = car_data_df['Accidents'].str.split().str[0]\n",
    "car_data_df['Owners'] = car_data_df['Owners'].str.split().str[0]\n",
    "car_data_df[['City_Eff', 'Highway_Eff']] = car_data_df['Fuel Efficiency'].str.split(' / ', expand=True)\n",
    "car_data_df['City_Eff'] = car_data_df['City_Eff'].str.split().str[0]\n",
    "car_data_df['Highway_Eff'] = car_data_df['Highway_Eff'].str.split().str[0]\n",
    "car_data_df['List_Date_Ago'] = car_data_df['Listed'].str.split().str[0]\n",
    "\n",
    "# Removing the comma and dollar sign from price to change it to float fom object\n",
    "# For listings with no Price, keeping the entry as NaN for numeric consistency.\n",
    "car_data_df['Price'] = car_data_df['Price'].replace('No Estimate Available', None).str.replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "# Arrange the Column for easier readability\n",
    "car_data_df = car_data_df[['SNum','Year', 'Brand', 'Model', 'Adv_Model', 'AWD', 'Miles', 'Price', 'Color', 'City', 'State', \n",
    "                               'List_Date_Ago', 'Accidents', 'Owners', 'Fuel_Type', 'City_Eff', 'Highway_Eff', 'Car_Title', 'VIN', 'URL']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Exporting to SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the final table to SQL on a local server, contained on Docker.\n",
    "# A database needs to be created on SQL before runnning it.\n",
    "# Create engine establishes the connection to write data on the specified database.\n",
    "\n",
    "engine = create_engine (\"mssql+pyodbc://SA:Password123@localhost/Github_Projects?driver=ODBC+Driver+18+for+SQL+Server&Encrypt=yes&TrustServerCertificate=yes\")\n",
    "car_data_df.to_sql(f\"{car_brand}_{car_model}\".replace(\"-\", \"_\"), engine, if_exists='replace', index=False)\n",
    "engine.dispose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
